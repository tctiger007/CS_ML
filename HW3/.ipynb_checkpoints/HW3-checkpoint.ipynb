{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL OF THE FOLLOWING VARIABLES ARE SUGGESTIONS, YOU MAY NOT NEED TO USE ALL OF THESE\n",
    "#ALSO, YOU MAY ADD MORE VARIABLES AS MAKE SENSE\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "#sizes = list showing the number of nodes/layer and number of layers. For example, \n",
    "#[2,5] means 2 hidden layers and first hidden layer has 2 nodes and the second hidden layer has 5 nodes\n",
    "    def __init__(self, sizes):\n",
    "        self.numLayers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.rand(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.rand(y,x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b)\n",
    "        return a\n",
    "    \n",
    "    def backprop(self,x,y):\n",
    "        #gradient descent layer by layer \n",
    "        gdLbL_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gdLbL_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #activation function \n",
    "        a = x \n",
    "        a_s = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w,a)+b   ############\n",
    "            zs.append(z)\n",
    "            a = sigmoid(z)\n",
    "            a_s.append(a)\n",
    "        #delta in backpropagation\n",
    "        d = (a_s[-1]-y) * sigmoid_derivative(zs[-1])\n",
    "        gdLbL_b[-1] = d\n",
    "        gdLbL_w[-1] = np.dot(d, a_s[-2].transpose())\n",
    "        for i in range(2,self.numLayers):\n",
    "            z=zs[-i]\n",
    "            d = np.dot(self.weights[-i+1].transpose(),d) * sigmoid_derivative(z)\n",
    "            gdLbL_b[-i] = d\n",
    "            gdLbL_w[-i] = np.dot(d,a[-i-1].transpose())\n",
    "        return (gdLbL_b,gdLbL_w)\n",
    "\n",
    "    def train(self,training_data, learningRate=0.001, maxIterations=10000, tol=0.0001):\n",
    "        #The training_data is a list of tuples (x, y)\n",
    "        gdLbL_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gdLbL_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in training_data:\n",
    "            d_gdLbL_b, d_gdLbL_w = self.backprop(x, y)\n",
    "            gdLbL_b = [nb+dnb for nb, dnb in zip(gdLbL_b, d_gdLbL_b)]\n",
    "            gdLbL_w = [nw+dnw for nw, dnw in zip(gdLbL_w, d_gdLbL_w)]\n",
    "        for w, nw in zip(self.weights, gdLbL_w):\n",
    "            self.weights = w-(learningRate/len(training_data))*nw \n",
    "        if self.weights[maxIterations] - self.weights[maxIterations-1] > tol:\n",
    "            print('Hit the max iterations')\n",
    "        self.biases = [b-(learningRate/len(training_data))*nb for b, nb in zip(self.biases, gdLbL_b)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
